{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import create_eva_vit_g\n",
    "# model = create_eva_vit_g(precision=\"fp16\").cuda()\n",
    "# import torch\n",
    "# with torch.cuda.amp.autocast():\n",
    "#     inputs = torch.randn(5, 3, 224, 224).cuda()\n",
    "#     result = model(inputs)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from box import Box\n",
    "\n",
    "with open('/home/huimingsun/Desktop/NGP/video_sgements/config/config.json', 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "config = Box(config_dict)\n",
    "\n",
    "# Now you can access values in the config like this:\n",
    "vit_model = config.model.vit_model\n",
    "vit_model_path = config.model.vit_model_path\n",
    "# ... and so on for the other values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast as autocast\n",
    "import torch.nn as nn\n",
    "from eva_vit import Trans_Block\n",
    "from recurrent_memory_transformer_pytorch import RecurrentMemoryTransformer\n",
    "\n",
    "try:\n",
    "    from .blip2 import Blip2Base, disabled_train\n",
    "except:\n",
    "    from blip2 import Blip2Base, disabled_train\n",
    "from einops import rearrange\n",
    "\n",
    "class VSeg(Blip2Base):\n",
    "    \"\"\"\n",
    "    VideoChat model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.low_resource = config.low_resource\n",
    "\n",
    "        self.vit_precision = config.vit_precision\n",
    "        print(f'Loading VIT. Use fp16: {config.vit_precision}')\n",
    "        self.visual_encoder, self.ln_vision = self.init_vision_encoder(\n",
    "            config.vit_model, config.img_size, config.drop_path_rate, \n",
    "            config.use_grad_checkpoint, config.vit_precision, config.vit_model_path,\n",
    "            temporal_downsample=config.temporal_downsample,\n",
    "            no_lmhra=config.no_lmhra, \n",
    "            double_lmhra=config.double_lmhra,\n",
    "            lmhra_reduction=config.lmhra_reduction, \n",
    "            gmhra_layers=config.gmhra_layers, \n",
    "            gmhra_drop_path_rate=config.gmhra_drop_path_rate,\n",
    "            gmhra_dropout=config.gmhra_dropout, \n",
    "        )\n",
    "        if config.freeze_vit:\n",
    "            print(\"freeze vision encoder\")\n",
    "            if not config.freeze_mhra:\n",
    "                open_list = []\n",
    "                for name, param in self.visual_encoder.named_parameters():\n",
    "                    if 'mhra' not in name:\n",
    "                        param.requires_grad = False\n",
    "                    else:\n",
    "                        open_list.append(name)\n",
    "\n",
    "            else:\n",
    "                for name, param in self.visual_encoder.named_parameters():\n",
    "                    param.requires_grad = False\n",
    "                self.visual_encoder = self.visual_encoder.eval()\n",
    "                self.visual_encoder.train = disabled_train\n",
    "                for name, param in self.ln_vision.named_parameters():\n",
    "                    param.requires_grad = False\n",
    "                self.ln_vision = self.ln_vision.eval()\n",
    "                self.ln_vision.train = disabled_train\n",
    "                \n",
    "        self.RMT = RecurrentMemoryTransformer(\n",
    "            num_tokens = 512,               # number of tokens\n",
    "            num_memory_tokens = 128,          # number of memory tokens, this will determine the bottleneck for information being passed to the future\n",
    "            dim = 512,                        # model dimensions\n",
    "            depth = 6,                        # transformer depth\n",
    "            causal = True,                    # autoregressive or not\n",
    "            dim_head = 64,                    # dimension per head\n",
    "            heads = 8,                        # heads\n",
    "            seq_len = 512*10,                   # sequence length of a segment\n",
    "            use_flash_attn = True             # whether to use flash attention\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "                Trans_Block(dim = 512, num_heads = config.embedding_size//88, mlp_ratio= 4.3637)\n",
    "                    for i in range(10)])\n",
    "        self.norm = nn.LayerNorm(config.embedding_size)\n",
    "        self.fc_norm = nn.LayerNorm(config.embedding_size)\n",
    "        self.fn1 =nn.Linear(1408,512)  #feature number from 1408 to 512\n",
    "        self.fn2 = nn.Linear(1286,512) #patch number from 1286 to 512\n",
    "        \n",
    "        self.fn3 = nn.Linear(512*10,512)\n",
    "        self.memory = None\n",
    "        self.head = nn.Linear(512, config.frames)\n",
    "        self.score_head = nn.Linear(config.embedding_size, 1)\n",
    "        self.max_txt_len = config.max_txt_len\n",
    "        self.cache_data = []\n",
    "\n",
    "    def vit_to_cpu(self):\n",
    "        self.ln_vision.to(\"cpu\")\n",
    "        self.ln_vision.float()\n",
    "        self.visual_encoder.to(\"cpu\")\n",
    "        self.visual_encoder.float()\n",
    "\n",
    "    def forward_all_features(self, interval_1,interval_2):\n",
    "\n",
    "        with self.maybe_autocast():\n",
    "            T = interval_1.shape[1]\n",
    "            # use_image = True if T == 1 else False\n",
    "            interval_1 = interval_1.permute(0, 2, 1, 3, 4) # [B,T,C,H,W] -> [B,C,T,H,W]\n",
    "            interval_2 = interval_2.permute(0, 2, 1, 3, 4) # [B,T,C,H,W] -> [B,C,T,H,W]\n",
    "            interval1_embeds = self.ln_vision(self.visual_encoder(interval_1))\n",
    "            interval2_embeds = self.ln_vision(self.visual_encoder(interval_2))\n",
    "        x = torch.concat((interval1_embeds, interval2_embeds), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward_feature(self, interval_1):\n",
    "\n",
    "        with self.maybe_autocast( dtype=torch.float16):\n",
    "            T = interval_1.shape[1]\n",
    "            # use_image = True if T == 1 else False\n",
    "            interval_1 = interval_1.permute(0, 2, 1, 3, 4) # [B,T,C,H,W] -> [B,C,T,H,W]\n",
    "            interval1_embeds = self.ln_vision(self.visual_encoder(interval_1))\n",
    "        return interval1_embeds\n",
    "\n",
    "            \n",
    "    def NoRMT_forward(self, interval_1,interval_2, y = None):\n",
    "        \n",
    "        x = self.forward_features(interval_1,interval_2)\n",
    "        print(x.shape)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        print(x.shape)\n",
    "        x = x[:, 0]\n",
    "        x = self.head(x)\n",
    "        if y!= None:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(x, y)\n",
    "            return x, loss\n",
    "        else:\n",
    "            return x     \n",
    "        \n",
    "        \n",
    "    def RMT_forward(self, x):\n",
    "\n",
    "        #[1, 512, 512]\n",
    "        print('RMT inputL ', x.shape)\n",
    "        RMT_output, self.memory, _ = self.RMT(x,self.memory)        # (1, 1024, 20000), (1, 128, 512), None\n",
    "        return  RMT_output\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.cache_data = []\n",
    "        self.memory = None\n",
    "    \n",
    "    def forward(self, interval_1, y = None):\n",
    "        \n",
    "        vit_feature = self.forward_feature(interval_1)\n",
    "        vit_feature = self.norm(vit_feature)\n",
    "\n",
    "        vit_feature = self.fn1(vit_feature)\n",
    "        vit_feature = rearrange(vit_feature, 'b c l -> b l c') # (patch number, feature vector) -> ( feature vector, patch number)\n",
    "        vit_feature = self.fn2(vit_feature)                    # decrease the patch number from 1286 to 512\n",
    "        vit_feature = rearrange(vit_feature, 'b l c -> b c l' ) # ( feature vector, patch number) -> (patch number, feature vector) \n",
    "          \n",
    "\n",
    "\n",
    "        if len(self.cache_data) == 0:\n",
    "            self.cache_data.append(vit_feature)\n",
    "            Prev_feature = torch.zeros_like(vit_feature)\n",
    "        else:   \n",
    "            Prev_feature = self.cache_data.pop()\n",
    "            self.cache_data.append(vit_feature)\n",
    "\n",
    "        RMT_feature = self.RMT_forward(vit_feature)\n",
    "\n",
    "        x = torch.cat((RMT_feature, Prev_feature, vit_feature), dim = 1)\n",
    "        # print('combined_feature: ', x.shape)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        # print(x.shape)\n",
    "        x = x[:, 0]\n",
    "        x = self.head(x)\n",
    "        if y!= None:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(x, y)\n",
    "            return x, loss\n",
    "        else:\n",
    "            return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from box import Box\n",
    "\n",
    "with open('/mnt/drive1/hsun/videoSeg/config/config.json', 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "config = Box(config_dict)\n",
    "model = VSeg(config.model).cuda()\n",
    "video = torch.rand(1, 5, 3,  224, 224).cuda()\n",
    "output = model(video)\n",
    "output = model(video)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT. Use fp16: fp32\n",
      "Temporal downsample: False\n",
      "freeze vision encoder\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current vit feature after linear : torch.Size([1, 512, 512])\n",
      "RMT inputL  torch.Size([1, 512, 512])\n",
      "RMT output:  torch.Size([1, 512, 512]) Prev_feature:  torch.Size([1, 512, 512]) vit_feature:  torch.Size([1, 512, 512]) Memory shape:  torch.Size([1, 128, 512])\n",
      "combined_feature:  torch.Size([1, 1536, 512])\n",
      "torch.Size([1, 1536, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5120])\n",
      "torch.Size([1, 5120, 512]) torch.Size([1, 128, 512])\n",
      "torch.Size([1, 5120, 512]) torch.Size([1, 128, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, 256, (1, 512*10)).cuda()\n",
    "print(x.shape)\n",
    "# x = torch.randint(0, 256, ( 197, 768))\n",
    "# print(x.shape)\n",
    "logits1, mem, _ = model(x)        # (1, 1024, 20000), (1, 128, 512), None\n",
    "print(logits1.shape, mem.shape)\n",
    "logits1, mem, _ = model(x, mem)  # (1, 1024, 20000), (1, 128, 512), None\n",
    "logits1, mem, _ = model(x, mem)  # (1, 1024, 20000), (1, 128, 512), None\n",
    "print(logits1.shape, mem.shape)\n",
    "\n",
    "# and so on ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from recurrent_memory_transformer_pytorch import RecurrentMemoryTransformer\n",
    "\n",
    "model = RecurrentMemoryTransformer(\n",
    "    num_tokens = 512,               # number of tokens\n",
    "    num_memory_tokens = 128,          # number of memory tokens, this will determine the bottleneck for information being passed to the future\n",
    "    dim = 512,                        # model dimensions\n",
    "    depth = 6,                        # transformer depth\n",
    "    causal = True,                    # autoregressive or not\n",
    "    dim_head = 64,                    # dimension per head\n",
    "    heads = 8,                        # heads\n",
    "    seq_len = 512,                   # sequence length of a segment\n",
    "    use_flash_attn = True             # whether to use flash attention\n",
    ").cuda()\n",
    "\n",
    "x = torch.randn((1, 512,512)).cuda()\n",
    "# x  = torch.randint(0, 256, (1, 1024)).cuda()\n",
    "\n",
    "# print(x.shape,x1.shape, x.dtype, x1.dtype)\n",
    "\n",
    "# and so on ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits1, mem1, _ = model(x)        # (1, 1024, 20000), (1, 128, 512), None\n",
    "logits2, mem2, _ = model(x, mem1)  # (1, 1024, 20000), (1, 128, 512), None\n",
    "logits3, mem3, _ = model(x, mem2)  # (1, 1024, 20000), (1, 128, 512), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
