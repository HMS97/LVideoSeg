{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [0,1,2,3]\n",
    "\n",
    "A.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2, device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from focal_loss import focal_loss\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "A = torch.from_numpy(np.array([1,0,1,0,1])).cuda()\n",
    "B = torch.from_numpy(np.array([1,0,0,1,0])).cuda()\n",
    "\n",
    "\n",
    "\n",
    "(A == B).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "weights = torch.tensor(1/100)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931)\n",
      "tensor(0.5412)\n",
      "tensor(0.9412)\n",
      "tensor(0.6172)\n"
     ]
    }
   ],
   "source": [
    "prediction = [0,0,0,0,0]\n",
    "gt = [0,0,0,0,0]\n",
    "\n",
    "print(loss_fn(torch.tensor(prediction).float(),torch.tensor(gt).float()))\n",
    "#tensor(0.6931)\n",
    "\n",
    "prediction = [0,1,0,1,0]\n",
    "gt = [0,1,0,1,0]\n",
    "\n",
    "print(loss_fn(torch.tensor(prediction).float(),torch.tensor(gt).float()))\n",
    "#tensor(0.5412)\n",
    "\n",
    "\n",
    "\n",
    "prediction = [0,1,0,1,0]\n",
    "gt = [0,0,0,0,0]\n",
    "\n",
    "print(loss_fn(torch.tensor(prediction).float(),torch.tensor(gt).float()))\n",
    "\n",
    "\n",
    "prediction = [0,1,0,0,0]\n",
    "gt = [0,1,0,1,0]\n",
    "\n",
    "print(loss_fn(torch.tensor(prediction).float(),torch.tensor(gt).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(100.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assume input (probabilities) and target (ground truth) are for batch size = 1\n",
    "input = torch.tensor([[0.0, 0.0, 0.0, 0.0, 0.0]]).float()\n",
    "target = torch.tensor([[0, 1, 0, 0, 0]]).float()\n",
    "\n",
    "# Define a weight for the positive class (1)\n",
    "pos_weight = torch.tensor([5.0])  # adjust this value as needed\n",
    "\n",
    "criterion = nn.BCELoss(weight=pos_weight)\n",
    "loss = criterion(input, target)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(78.8084)\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([[0.1, 0.7, 0.2, 0.1, 0.1]]).float()\n",
    "target = torch.tensor([[0, 0, 0, 1, 0]]).float()\n",
    "\n",
    "# Define a weight for the positive class (1)\n",
    "pos_weight = torch.tensor([100.0])  # adjust this value as needed\n",
    "\n",
    "criterion = nn.BCELoss(weight=pos_weight)\n",
    "loss = criterion(input, target)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from path import Path\n",
    "\n",
    "pp = Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos').dirs()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1013.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1014.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1015.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1016.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1017.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1018.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1019.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1020.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1021.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1022.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1023.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1024.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1025.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1026.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1027.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1028.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1029.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1030.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1031.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1032.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1033.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1034.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1035.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1036.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1037.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1038.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1039.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1040.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1041.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1042.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1043.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1044.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1045.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1046.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1047.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1048.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1049.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1050.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1051.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1052.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1053.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1054.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1055.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1056.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1057.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1058.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1059.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1060.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1061.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1062.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1063.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1064.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1065.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1066.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1067.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1068.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1069.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1070.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1071.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1072.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1073.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1074.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1075.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1076.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1077.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1078.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1079.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1080.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1081.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1082.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1083.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1084.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1085.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1086.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1087.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1088.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1089.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1090.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1091.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1092.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1093.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1094.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1095.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1096.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1097.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1098.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1099.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1100.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1101.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1102.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1103.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1104.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1105.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1106.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1107.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1108.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1109.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1110.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1111.jpg'),\n",
       " Path('/mnt/drive1/brick1/hsun/videoSeg/data/video_datasets/CondensedMovies/new_videos/2498/frame_1112.jpg')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(pp.files('*.jpg'), key=lambda x:  int(x.stem.split('_')[-1]))[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5369)\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([[0.1, 0.9, 0.2, 0.1, 0.1]]).float()\n",
    "target = torch.tensor([[0, 0, 1, 0, 0]]).float()\n",
    "\n",
    "# Define a weight for the positive class (1)\n",
    "pos_weight = torch.tensor([3.0])  # adjust this value as needed\n",
    "\n",
    "criterion = nn.BCELoss(weight=pos_weight)\n",
    "loss = criterion(input, target)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from box import Box\n",
    "\n",
    "with open('/home/huimingsun/Desktop/NGP/video_sgements/config/config.json', 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "config = Box(config_dict)\n",
    "\n",
    "# Now you can access values in the config like this:\n",
    "vit_model = config.model.vit_model\n",
    "vit_model_path = config.model.vit_model_path\n",
    "# ... and so on for the other values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast as autocast\n",
    "import torch.nn as nn\n",
    "from eva_vit import Trans_Block\n",
    "from recurrent_memory_transformer_pytorch import RecurrentMemoryTransformer\n",
    "\n",
    "try:\n",
    "    from .blip2 import Blip2Base, disabled_train\n",
    "except:\n",
    "    from blip2 import Blip2Base, disabled_train\n",
    "from einops import rearrange\n",
    "\n",
    "class VSeg(Blip2Base):\n",
    "    \"\"\"\n",
    "    VideoChat model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.low_resource = config.low_resource\n",
    "\n",
    "        self.vit_precision = config.vit_precision\n",
    "        print(f'Loading VIT. Use fp16: {config.vit_precision}')\n",
    "        self.visual_encoder, self.ln_vision = self.init_vision_encoder(\n",
    "            config.vit_model, config.img_size, config.drop_path_rate, \n",
    "            config.use_grad_checkpoint, config.vit_precision, config.vit_model_path,\n",
    "            temporal_downsample=config.temporal_downsample,\n",
    "            no_lmhra=config.no_lmhra, \n",
    "            double_lmhra=config.double_lmhra,\n",
    "            lmhra_reduction=config.lmhra_reduction, \n",
    "            gmhra_layers=config.gmhra_layers, \n",
    "            gmhra_drop_path_rate=config.gmhra_drop_path_rate,\n",
    "            gmhra_dropout=config.gmhra_dropout, \n",
    "        )\n",
    "        if config.freeze_vit:\n",
    "            print(\"freeze vision encoder\")\n",
    "            if not config.freeze_mhra:\n",
    "                open_list = []\n",
    "                for name, param in self.visual_encoder.named_parameters():\n",
    "                    if 'mhra' not in name:\n",
    "                        param.requires_grad = False\n",
    "                    else:\n",
    "                        open_list.append(name)\n",
    "\n",
    "            else:\n",
    "                for name, param in self.visual_encoder.named_parameters():\n",
    "                    param.requires_grad = False\n",
    "                self.visual_encoder = self.visual_encoder.eval()\n",
    "                self.visual_encoder.train = disabled_train\n",
    "                for name, param in self.ln_vision.named_parameters():\n",
    "                    param.requires_grad = False\n",
    "                self.ln_vision = self.ln_vision.eval()\n",
    "                self.ln_vision.train = disabled_train\n",
    "                \n",
    "        self.RMT = RecurrentMemoryTransformer(\n",
    "            num_tokens = 512,               # number of tokens\n",
    "            num_memory_tokens = 128,          # number of memory tokens, this will determine the bottleneck for information being passed to the future\n",
    "            dim = 512,                        # model dimensions\n",
    "            depth = 6,                        # transformer depth\n",
    "            causal = True,                    # autoregressive or not\n",
    "            dim_head = 64,                    # dimension per head\n",
    "            heads = 8,                        # heads\n",
    "            seq_len = 512*10,                   # sequence length of a segment\n",
    "            use_flash_attn = True             # whether to use flash attention\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "                Trans_Block(dim = 512, num_heads = config.embedding_size//88, mlp_ratio= 4.3637)\n",
    "                    for i in range(10)])\n",
    "        self.norm = nn.LayerNorm(config.embedding_size)\n",
    "        self.fc_norm = nn.LayerNorm(config.embedding_size)\n",
    "        self.fn1 =nn.Linear(1408,512)  #feature number from 1408 to 512\n",
    "        self.fn2 = nn.Linear(1286,512) #patch number from 1286 to 512\n",
    "        \n",
    "        self.fn3 = nn.Linear(512*10,512)\n",
    "        self.memory = None\n",
    "        self.head = nn.Linear(512, config.frames)\n",
    "        self.score_head = nn.Linear(config.embedding_size, 1)\n",
    "        self.max_txt_len = config.max_txt_len\n",
    "        self.cache_data = []\n",
    "\n",
    "    def vit_to_cpu(self):\n",
    "        self.ln_vision.to(\"cpu\")\n",
    "        self.ln_vision.float()\n",
    "        self.visual_encoder.to(\"cpu\")\n",
    "        self.visual_encoder.float()\n",
    "\n",
    "    def forward_all_features(self, interval_1,interval_2):\n",
    "\n",
    "        with self.maybe_autocast():\n",
    "            T = interval_1.shape[1]\n",
    "            # use_image = True if T == 1 else False\n",
    "            interval_1 = interval_1.permute(0, 2, 1, 3, 4) # [B,T,C,H,W] -> [B,C,T,H,W]\n",
    "            interval_2 = interval_2.permute(0, 2, 1, 3, 4) # [B,T,C,H,W] -> [B,C,T,H,W]\n",
    "            interval1_embeds = self.ln_vision(self.visual_encoder(interval_1))\n",
    "            interval2_embeds = self.ln_vision(self.visual_encoder(interval_2))\n",
    "        x = torch.concat((interval1_embeds, interval2_embeds), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward_feature(self, interval_1):\n",
    "\n",
    "        with self.maybe_autocast( dtype=torch.float16):\n",
    "            T = interval_1.shape[1]\n",
    "            # use_image = True if T == 1 else False\n",
    "            interval_1 = interval_1.permute(0, 2, 1, 3, 4) # [B,T,C,H,W] -> [B,C,T,H,W]\n",
    "            interval1_embeds = self.ln_vision(self.visual_encoder(interval_1))\n",
    "        return interval1_embeds\n",
    "\n",
    "            \n",
    "    def NoRMT_forward(self, interval_1,interval_2, y = None):\n",
    "        \n",
    "        x = self.forward_features(interval_1,interval_2)\n",
    "        print(x.shape)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        print(x.shape)\n",
    "        x = x[:, 0]\n",
    "        x = self.head(x)\n",
    "        if y!= None:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(x, y)\n",
    "            return x, loss\n",
    "        else:\n",
    "            return x     \n",
    "        \n",
    "        \n",
    "    def RMT_forward(self, x):\n",
    "\n",
    "        #[1, 512, 512]\n",
    "        print('RMT inputL ', x.shape)\n",
    "        RMT_output, self.memory, _ = self.RMT(x,self.memory)        # (1, 1024, 20000), (1, 128, 512), None\n",
    "        return  RMT_output\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.cache_data = []\n",
    "        self.memory = None\n",
    "    \n",
    "    def forward(self, interval_1, y = None):\n",
    "        \n",
    "        vit_feature = self.forward_feature(interval_1)\n",
    "        vit_feature = self.norm(vit_feature)\n",
    "\n",
    "        vit_feature = self.fn1(vit_feature)\n",
    "        vit_feature = rearrange(vit_feature, 'b c l -> b l c') # (patch number, feature vector) -> ( feature vector, patch number)\n",
    "        vit_feature = self.fn2(vit_feature)                    # decrease the patch number from 1286 to 512\n",
    "        vit_feature = rearrange(vit_feature, 'b l c -> b c l' ) # ( feature vector, patch number) -> (patch number, feature vector) \n",
    "          \n",
    "\n",
    "\n",
    "        if len(self.cache_data) == 0:\n",
    "            self.cache_data.append(vit_feature)\n",
    "            Prev_feature = torch.zeros_like(vit_feature)\n",
    "        else:   \n",
    "            Prev_feature = self.cache_data.pop()\n",
    "            self.cache_data.append(vit_feature)\n",
    "\n",
    "        RMT_feature = self.RMT_forward(vit_feature)\n",
    "\n",
    "        x = torch.cat((RMT_feature, Prev_feature, vit_feature), dim = 1)\n",
    "        # print('combined_feature: ', x.shape)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        # print(x.shape)\n",
    "        x = x[:, 0]\n",
    "        x = self.head(x)\n",
    "        if y!= None:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(x, y)\n",
    "            return x, loss\n",
    "        else:\n",
    "            return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from box import Box\n",
    "\n",
    "with open('/mnt/drive1/hsun/videoSeg/config/config.json', 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "config = Box(config_dict)\n",
    "model = VSeg(config.model).cuda()\n",
    "video = torch.rand(1, 5, 3,  224, 224).cuda()\n",
    "output = model(video)\n",
    "output = model(video)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT. Use fp16: fp32\n",
      "Temporal downsample: False\n",
      "freeze vision encoder\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current vit feature after linear : torch.Size([1, 512, 512])\n",
      "RMT inputL  torch.Size([1, 512, 512])\n",
      "RMT output:  torch.Size([1, 512, 512]) Prev_feature:  torch.Size([1, 512, 512]) vit_feature:  torch.Size([1, 512, 512]) Memory shape:  torch.Size([1, 128, 512])\n",
      "combined_feature:  torch.Size([1, 1536, 512])\n",
      "torch.Size([1, 1536, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5120])\n",
      "torch.Size([1, 5120, 512]) torch.Size([1, 128, 512])\n",
      "torch.Size([1, 5120, 512]) torch.Size([1, 128, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, 256, (1, 512*10)).cuda()\n",
    "print(x.shape)\n",
    "# x = torch.randint(0, 256, ( 197, 768))\n",
    "# print(x.shape)\n",
    "logits1, mem, _ = model(x)        # (1, 1024, 20000), (1, 128, 512), None\n",
    "print(logits1.shape, mem.shape)\n",
    "logits1, mem, _ = model(x, mem)  # (1, 1024, 20000), (1, 128, 512), None\n",
    "logits1, mem, _ = model(x, mem)  # (1, 1024, 20000), (1, 128, 512), None\n",
    "print(logits1.shape, mem.shape)\n",
    "\n",
    "# and so on ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from recurrent_memory_transformer_pytorch import RecurrentMemoryTransformer\n",
    "\n",
    "model = RecurrentMemoryTransformer(\n",
    "    num_tokens = 512,               # number of tokens\n",
    "    num_memory_tokens = 128,          # number of memory tokens, this will determine the bottleneck for information being passed to the future\n",
    "    dim = 512,                        # model dimensions\n",
    "    depth = 6,                        # transformer depth\n",
    "    causal = True,                    # autoregressive or not\n",
    "    dim_head = 64,                    # dimension per head\n",
    "    heads = 8,                        # heads\n",
    "    seq_len = 512,                   # sequence length of a segment\n",
    "    use_flash_attn = True             # whether to use flash attention\n",
    ").cuda()\n",
    "\n",
    "x = torch.randn((1, 512,512)).cuda()\n",
    "# x  = torch.randint(0, 256, (1, 1024)).cuda()\n",
    "\n",
    "# print(x.shape,x1.shape, x.dtype, x1.dtype)\n",
    "\n",
    "# and so on ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits1, mem1, _ = model(x)        # (1, 1024, 20000), (1, 128, 512), None\n",
    "logits2, mem2, _ = model(x, mem1)  # (1, 1024, 20000), (1, 128, 512), None\n",
    "logits3, mem3, _ = model(x, mem2)  # (1, 1024, 20000), (1, 128, 512), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
